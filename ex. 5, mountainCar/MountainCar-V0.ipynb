{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите агента, набирающего как можно больше баллов в игре MountainCar-V0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы решения "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем решить задачу двумя способами и сравнить результаты:\n",
    "- Сделаем выборку из большого числа \"успешных\" игр и обучим нейронную сеть\n",
    "    - Так как мы получаем награду, отличную от $-1$ только в том случае, когда машинка пересекает отметку в $0.5$ (что при случайных действиях случается экстремально редко), изменим функцию награды так, что значение награды за итерацию увеличивается на $1$, если мы достигли отметку в $-0.18$ (это значение нам подойдёт, так как стартовая позиция принимает значения от $-0.6$ до $-0.4$).\n",
    "    - Игра будет считаться успешной, если за $200$ максимально возможных итераций суммарное значение награды составило больше $-199$.\n",
    "- DQN\n",
    "    - Мы не будем просто собирать данные и обучать нейронную сеть на их основе. Вместо этого мы будем создавать тренировочные данные прямо после каждой попытки и будем обучать сеть сразу же после попытки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from collections import deque\n",
    "from tensorflow.keras.models     import Sequential\n",
    "from tensorflow.keras.layers     import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала напишем метод, который будет сравнивать эффективность моделей. Он будет принимать модель, которая играет $100$ игр и считает среднее количество награды за игру (расчёт награды не меняется относительно условия задачи)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(test_model):\n",
    "    \n",
    "    # Список из суммарных наград за 1 игру.\n",
    "    scores = [] \n",
    "    \n",
    "    games_number = 100\n",
    "    \n",
    "    # Пробегаемся по 100 играм.\n",
    "    for each_game in range(games_number):    \n",
    "        \n",
    "        score = 0\n",
    "        \n",
    "        # Состояние среды за предыдущую итерацию. \n",
    "        # Будем его передавать в качестве параметра для нашей модели.\n",
    "        previous_observation = []\n",
    "        \n",
    "        for step_index in range(goal_steps):\n",
    "            \n",
    "            # Если это первая итерация, то выбераем случайное действие.\n",
    "            if len(previous_observation)==0:\n",
    "                action = random.randrange(0,2)\n",
    "            # Иначе подаём модели прошлой состояние и выбираем действие с наибольшим весом.\n",
    "            else:\n",
    "                action = np.argmax(test_model.predict(previous_observation.reshape(-1, env.observation_space.shape[0]))[0])\n",
    "\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            previous_observation = new_observation\n",
    "            score += reward\n",
    "            \n",
    "            if done:            \n",
    "                break\n",
    "                \n",
    "        env.reset()\n",
    "        scores.append(score)\n",
    "            \n",
    "    print('Средняя награда за {} игр: {}'.format(games_number, sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение №1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс, содержащий всю логику решения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "        self.goal_steps = 200\n",
    "        self.score_requirement = -199\n",
    "        self.games_number = 100000\n",
    "\n",
    "        self.model = self.create_model()\n",
    "\n",
    "    # Создание модели.\n",
    "    def create_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(512, input_dim=self.env.observation_space.shape[0], activation='relu'),\n",
    "            Dense(256, input_dim=input_size, activation='relu'),\n",
    "            Dense(128, input_dim=input_size, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.env.action_space.n, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=Adam(), metrics=['acc'])\n",
    "        return model\n",
    "    \n",
    "    # Обучение модели.\n",
    "    def train_model(self, x, y):        \n",
    "        self.model.fit(x, y, epochs=5)\n",
    "\n",
    "    # Подготовка данных для обучения.\n",
    "    def data_preparation(self):\n",
    "        \n",
    "        # Сюда будем записывать информацию об играх, которые прошли отбор по награде.\n",
    "        training_data = []\n",
    "        \n",
    "        # Сюда будем записывать количество очков в играх, которые прошли отбор.\n",
    "        accepted_scores = []\n",
    "        \n",
    "        # Пробегаемся по всем играм.\n",
    "        for game_index in range(self.games_number):\n",
    "            \n",
    "            score = 0\n",
    "            \n",
    "            # Сюда запишем всю историю\n",
    "            game_memory = []\n",
    "            previous_observation = []\n",
    "            \n",
    "            for step_index in range(self.goal_steps):\n",
    "                action = random.randrange(0, 3)\n",
    "                observation, reward, done, info = env.step(action)\n",
    "\n",
    "                if len(previous_observation) > 0:\n",
    "                    game_memory.append([previous_observation, action])\n",
    "\n",
    "                previous_observation = observation\n",
    "                if observation[0] > -0.18:\n",
    "                    reward = 1\n",
    "\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Если к концу игры мы набрали нужное количество награды, то \n",
    "            # все состояния и принятые решение записываем в training data.\n",
    "            if score >= self.score_requirement:\n",
    "                accepted_scores.append(score)\n",
    "                for data in game_memory:\n",
    "                    if data[1] == 1:\n",
    "                        output = [0, 1, 0]\n",
    "                    elif data[1] == 0:\n",
    "                        output = [1, 0, 0]\n",
    "                    elif data[1] == 2:\n",
    "                        output = [0, 0, 1]\n",
    "                    training_data.append([data[0], output])\n",
    "\n",
    "            env.reset()\n",
    "            \n",
    "            if (game_index%1000==0):\n",
    "                print('Завершено {} игр из {}.'.format(game_index, self.games_number))\n",
    "\n",
    "        print(\"Количество игр, данные по которым будут использованы для обучения: \",len(accepted_scores))\n",
    "        \n",
    "        # Составляем массивы для обучения (x – состояние, y – решение)\n",
    "        x = np.array([i[0] for i in training_data]).reshape(-1, self.env.observation_space.shape[0])\n",
    "        y = np.array([i[1] for i in training_data]).reshape(-1, self.env.action_space.n)\n",
    "\n",
    "        return x, y\n",
    "    def save_model(self, filepath):\n",
    "        self.model.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем данный метод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43422332,  0.        ])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_agent = NN(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Завершено 0 игр из 100000.\n",
      "Завершено 1000 игр из 100000.\n",
      "Завершено 2000 игр из 100000.\n",
      "Завершено 3000 игр из 100000.\n",
      "Завершено 4000 игр из 100000.\n",
      "Завершено 5000 игр из 100000.\n",
      "Завершено 6000 игр из 100000.\n",
      "Завершено 7000 игр из 100000.\n",
      "Завершено 8000 игр из 100000.\n",
      "Завершено 9000 игр из 100000.\n",
      "Завершено 10000 игр из 100000.\n",
      "Завершено 11000 игр из 100000.\n",
      "Завершено 12000 игр из 100000.\n",
      "Завершено 13000 игр из 100000.\n",
      "Завершено 14000 игр из 100000.\n",
      "Завершено 15000 игр из 100000.\n",
      "Завершено 16000 игр из 100000.\n",
      "Завершено 17000 игр из 100000.\n",
      "Завершено 18000 игр из 100000.\n",
      "Завершено 19000 игр из 100000.\n",
      "Завершено 20000 игр из 100000.\n",
      "Завершено 21000 игр из 100000.\n",
      "Завершено 22000 игр из 100000.\n",
      "Завершено 23000 игр из 100000.\n",
      "Завершено 24000 игр из 100000.\n",
      "Завершено 25000 игр из 100000.\n",
      "Завершено 26000 игр из 100000.\n",
      "Завершено 27000 игр из 100000.\n",
      "Завершено 28000 игр из 100000.\n",
      "Завершено 29000 игр из 100000.\n",
      "Завершено 30000 игр из 100000.\n",
      "Завершено 31000 игр из 100000.\n",
      "Завершено 32000 игр из 100000.\n",
      "Завершено 33000 игр из 100000.\n",
      "Завершено 34000 игр из 100000.\n",
      "Завершено 35000 игр из 100000.\n",
      "Завершено 36000 игр из 100000.\n",
      "Завершено 37000 игр из 100000.\n",
      "Завершено 38000 игр из 100000.\n",
      "Завершено 39000 игр из 100000.\n",
      "Завершено 40000 игр из 100000.\n",
      "Завершено 41000 игр из 100000.\n",
      "Завершено 42000 игр из 100000.\n",
      "Завершено 43000 игр из 100000.\n",
      "Завершено 44000 игр из 100000.\n",
      "Завершено 45000 игр из 100000.\n",
      "Завершено 46000 игр из 100000.\n",
      "Завершено 47000 игр из 100000.\n",
      "Завершено 48000 игр из 100000.\n",
      "Завершено 49000 игр из 100000.\n",
      "Завершено 50000 игр из 100000.\n",
      "Завершено 51000 игр из 100000.\n",
      "Завершено 52000 игр из 100000.\n",
      "Завершено 53000 игр из 100000.\n",
      "Завершено 54000 игр из 100000.\n",
      "Завершено 55000 игр из 100000.\n",
      "Завершено 56000 игр из 100000.\n",
      "Завершено 57000 игр из 100000.\n",
      "Завершено 58000 игр из 100000.\n",
      "Завершено 59000 игр из 100000.\n",
      "Завершено 60000 игр из 100000.\n",
      "Завершено 61000 игр из 100000.\n",
      "Завершено 62000 игр из 100000.\n",
      "Завершено 63000 игр из 100000.\n",
      "Завершено 64000 игр из 100000.\n",
      "Завершено 65000 игр из 100000.\n",
      "Завершено 66000 игр из 100000.\n",
      "Завершено 67000 игр из 100000.\n",
      "Завершено 68000 игр из 100000.\n",
      "Завершено 69000 игр из 100000.\n",
      "Завершено 70000 игр из 100000.\n",
      "Завершено 71000 игр из 100000.\n",
      "Завершено 72000 игр из 100000.\n",
      "Завершено 73000 игр из 100000.\n",
      "Завершено 74000 игр из 100000.\n",
      "Завершено 75000 игр из 100000.\n",
      "Завершено 76000 игр из 100000.\n",
      "Завершено 77000 игр из 100000.\n",
      "Завершено 78000 игр из 100000.\n",
      "Завершено 79000 игр из 100000.\n",
      "Завершено 80000 игр из 100000.\n",
      "Завершено 81000 игр из 100000.\n",
      "Завершено 82000 игр из 100000.\n",
      "Завершено 83000 игр из 100000.\n",
      "Завершено 84000 игр из 100000.\n",
      "Завершено 85000 игр из 100000.\n",
      "Завершено 86000 игр из 100000.\n",
      "Завершено 87000 игр из 100000.\n",
      "Завершено 88000 игр из 100000.\n",
      "Завершено 89000 игр из 100000.\n",
      "Завершено 90000 игр из 100000.\n",
      "Завершено 91000 игр из 100000.\n",
      "Завершено 92000 игр из 100000.\n",
      "Завершено 93000 игр из 100000.\n",
      "Завершено 94000 игр из 100000.\n",
      "Завершено 95000 игр из 100000.\n",
      "Завершено 96000 игр из 100000.\n",
      "Завершено 97000 игр из 100000.\n",
      "Завершено 98000 игр из 100000.\n",
      "Завершено 99000 игр из 100000.\n",
      "Количество игр, данные по которым будут использованы для обучения:  232\n"
     ]
    }
   ],
   "source": [
    "x, y = nn_agent.data_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "46168/46168 [==============================] - 7s 157us/sample - loss: 0.2213 - acc: 0.3787\n",
      "Epoch 2/5\n",
      "46168/46168 [==============================] - 8s 163us/sample - loss: 0.2193 - acc: 0.4005\n",
      "Epoch 3/5\n",
      "46168/46168 [==============================] - 7s 149us/sample - loss: 0.2188 - acc: 0.3998\n",
      "Epoch 4/5\n",
      "46168/46168 [==============================] - 7s 147us/sample - loss: 0.2184 - acc: 0.4027\n",
      "Epoch 5/5\n",
      "46168/46168 [==============================] - 7s 146us/sample - loss: 0.2184 - acc: 0.4021\n"
     ]
    }
   ],
   "source": [
    "nn_agent.train_model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя награда за 100 игр: -137.27\n"
     ]
    }
   ],
   "source": [
    "model_test(nn_agent.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_agent.save_model(\"NNmodel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение №2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс, содержащий всю логику решения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.env     = env\n",
    "        \n",
    "        # Сюда будем складывать информацию о всех играх.\n",
    "        self.memory  = deque(maxlen=2000)\n",
    "        \n",
    "        # Параметр, на который будет снижаться награда с последующими итерациями.\n",
    "        self.gamma = 0.85\n",
    "        \n",
    "        # Разброс коэффицента, в зависимости от которого мы будем совершать случайное\n",
    "        # действие в конкретном случае. Вначале он большой, так как у нас нет\n",
    "        # никакой информации о среде.\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        # Коэффицент, на который мы будем снижать эпсилон после каждой успешной итерации.\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        self.learning_rate = 0.005\n",
    "        \n",
    "        # Коэффицент для пересчёта весов model в веса target_model.\n",
    "        self.tau = .125\n",
    "\n",
    "        # model – непосредственно для предсказания шага, мы делаем.\n",
    "        # target_model – для предсказания шага, который мы хотим сделать.\n",
    "        self.model        = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    # Создание модели.\n",
    "    def create_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(24, input_dim=self.env.observation_space.shape[0], activation=\"relu\"),\n",
    "            Dense(48, activation=\"relu\"),\n",
    "            Dense(24, activation=\"relu\"),\n",
    "            Dense(self.env.action_space.n)\n",
    "        ])\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=Adam(lr=self.learning_rate), metrics=['acc'])\n",
    "        return model\n",
    "\n",
    "    # Метод, который выдаёт действие, которое мы в конечном итоге применяем.\n",
    "    # Будет ли действие случайным или же оно будет выдоно моделью зависит от\n",
    "    # текущих параметров эпсилон.\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    # Добавление итераций в память.\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    # Обучаем сеть на основе предыдущих итераций.\n",
    "    def replay(self):\n",
    "        batch_size = 32\n",
    "        \n",
    "        # Если у нас ещё недостаточно данных, ничего не делоем.\n",
    "        if len(self.memory) < batch_size: \n",
    "            return\n",
    "        \n",
    "        # Берём случайных срез данных.\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = self.target_model.predict(state)\n",
    "            \n",
    "            # Если мы дошли до последней итерации или дошли до значения 0.5, то\n",
    "            # мы больше не получим последующих наград.\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            \n",
    "            # Иначе мы хотем увидеть максимальную награду, которую мы бы получили\n",
    "            # если бы у нас была возможность совершить любое действие.\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "                \n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    # Копируем веса в target_model.\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "        \n",
    "    # Собираем всё воедино и обучаем модель.\n",
    "    def train_model(self):\n",
    "        \n",
    "        games_number  = 1000\n",
    "        goal_steps = 200\n",
    "        \n",
    "        steps = []\n",
    "        for game_index in range(games_number):\n",
    "\n",
    "            cur_state = env.reset().reshape(1,2)\n",
    "\n",
    "            for step_index in range(goal_steps):\n",
    "                \n",
    "                # Выбираем действие.\n",
    "                action = self.act(cur_state)\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # Запоминаем состояние.\n",
    "                new_state = new_state.reshape(1,2)\n",
    "                self.remember(cur_state, action, reward, new_state, done)\n",
    "\n",
    "                # Обновляем веса обоих моделей\n",
    "                self.replay()       \n",
    "                self.target_train() \n",
    "\n",
    "                cur_state = new_state\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "                if (step_index%50==0):\n",
    "                    print('Игра {}, итерация {}.'.format(game_index, step_index))\n",
    "                    \n",
    "            if step_index < 199:\n",
    "                print(\"Выполнение завершено за {} игр.\".format(game_index))\n",
    "                self.save_model(\"DQNmodel.h5\")\n",
    "                break\n",
    "                        \n",
    "            print('Завершено {} игр из {}.'.format(game_index, games_number))\n",
    "\n",
    "    # Сохраняем модель.\n",
    "    def save_model(self, filepath):\n",
    "        self.model.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43507919,  0.        ])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQN(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерация 0, игра 0.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-275-c326e35f10a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-270-e43171bc7e98>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;31m# Обновляем веса обоих моделей\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-270-e43171bc7e98>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;31m# Если мы дошли до последней итерации или дошли до значения 0.5, то\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       return training_arrays.predict_loop(\n\u001b[1;32m-> 1113\u001b[1;33m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;31m# Setup work for each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1117\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'metrics'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m         \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[0mtraining_distributed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m     \"\"\"\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m       \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mset_value\u001b[1;34m(x, value)\u001b[0m\n\u001b[0;32m   2845\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2846\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2847\u001b[1;33m       \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dqn_agent.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя награда за 100 игр: -198.01\n"
     ]
    }
   ],
   "source": [
    "model_test(dqn_agent.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
