{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[section title](#intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите агента, набирающего как можно больше баллов в игре MountainCar-V0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы решения "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем решить задачу двумя способами и сравнить результаты:\n",
    "- Сделаем выборку из большого числа \"успешных\" игр и обучим нейронную сеть\n",
    "    - Так как мы получаем награду, отличную от $-1$ только в том случае, когда машинка пересекает отметку в $0.5$ (что при случайных действиях случается экстремально редко), изменим функцию награды так, что значение награды за итерацию увеличивается на $1$, если мы достигли отметку в $-0.3$ (это значение нам подойдёт, так как стартовая позиция принимает значения от $-0.6$ до $-0.4$).\n",
    "    - Игра будет считаться успешной, если за $200$ максимально возможных итераций суммарное значение награды составило больше $-185$.\n",
    "- DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models     import Sequential\n",
    "from tensorflow.keras.layers     import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала напишем метод, который будет сравнивать эффективность моделей. Он будет принимать модель, которая играет $100$ игр и считает среднее количество награды за игру (расчёт награды не меняется относительно условия задачи)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(test_model):\n",
    "    \n",
    "    # Список из суммарных наград за 1 игру.\n",
    "    scores = [] \n",
    "    \n",
    "    games_number = 100\n",
    "    \n",
    "    # Пробегаемся по 100 играм.\n",
    "    for each_game in range(games_number):    \n",
    "        \n",
    "        score = 0\n",
    "        \n",
    "        # Состояние среды за предыдущую итерацию. \n",
    "        # Будем его передавать в качестве параметра для нашей модели.\n",
    "        previous_observation = []\n",
    "        \n",
    "        for step_index in range(goal_steps):\n",
    "            \n",
    "            # Если это первая итерация, то выбераем случайное действие.\n",
    "            if len(previous_observation)==0:\n",
    "                action = random.randrange(0,2)\n",
    "            # Иначе подаём модели прошлой состояние и выбираем действие с наибольшим весом.\n",
    "            else:\n",
    "                action = np.argmax(test_model.predict(previous_observation.reshape(-1, env.observation_space.shape[0]))[0])\n",
    "\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            previous_observation = new_observation\n",
    "            score += reward\n",
    "            \n",
    "            if done:            \n",
    "                break\n",
    "                \n",
    "        env.reset()\n",
    "        scores.append(score)\n",
    "            \n",
    "    print('Средняя награда за {} игр: {}'.format(games_number, sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение №1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс, содержащий всю логику решения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "        self.goal_steps = 200\n",
    "        self.score_requirement = -185\n",
    "        self.games_number = 10000\n",
    "\n",
    "        self.model = self.create_model()\n",
    "\n",
    "    # Создание модели.\n",
    "    def create_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(512, input_dim=self.env.observation_space.shape[0], activation='relu'),\n",
    "            Dense(256, input_dim=input_size, activation='relu'),\n",
    "            Dense(128, input_dim=input_size, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.env.action_space.n, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=Adam(), metrics=['acc'])\n",
    "        return model\n",
    "    \n",
    "    # Обучение модели.\n",
    "    def train_model(self, x, y):        \n",
    "        self.model.fit(x, y, epochs=5)\n",
    "\n",
    "    # Подготовка данных для обучения.\n",
    "    def data_preparation(self):\n",
    "        \n",
    "        # Сюда будем записывать информацию об играх, которые прошли отбор по награде.\n",
    "        training_data = []\n",
    "        \n",
    "        # Сюда будем записывать количество очков в играх, которые прошли отбор.\n",
    "        accepted_scores = []\n",
    "        \n",
    "        # Пробегаемся по всем играм.\n",
    "        for game_index in range(self.games_number):\n",
    "            \n",
    "            score = 0\n",
    "            \n",
    "            # Сюда запишем всю историю\n",
    "            game_memory = []\n",
    "            previous_observation = []\n",
    "            \n",
    "            for step_index in range(self.goal_steps):\n",
    "                action = random.randrange(0, 3)\n",
    "                observation, reward, done, info = env.step(action)\n",
    "\n",
    "                if len(previous_observation) > 0:\n",
    "                    game_memory.append([previous_observation, action])\n",
    "\n",
    "                previous_observation = observation\n",
    "                if observation[0] > -0.3:\n",
    "                    reward = 1\n",
    "\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Если к концу игры мы набрали нужное количество награды, то \n",
    "            # все состояния и принятые решение записываем в training data.\n",
    "            if score >= self.score_requirement:\n",
    "                accepted_scores.append(score)\n",
    "                for data in game_memory:\n",
    "                    if data[1] == 1:\n",
    "                        output = [0, 1, 0]\n",
    "                    elif data[1] == 0:\n",
    "                        output = [1, 0, 0]\n",
    "                    elif data[1] == 2:\n",
    "                        output = [0, 0, 1]\n",
    "                    training_data.append([data[0], output])\n",
    "\n",
    "            env.reset()\n",
    "            \n",
    "            if (game_index%1000==0):\n",
    "                print('Завершено {} итераций из {}.'.format(game_index, self.games_number))\n",
    "\n",
    "        print(\"Количество игр, данные по которым будут использованы для обучения: \",len(accepted_scores))\n",
    "        \n",
    "        # Составляем массивы для обучения (x – состояние, y – решение)\n",
    "        x = np.array([i[0] for i in training_data]).reshape(-1, self.env.observation_space.shape[0])\n",
    "        y = np.array([i[1] for i in training_data]).reshape(-1, self.env.action_space.n)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем данные метод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.52396908,  0.        ])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_agent = NN(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Завершено 0 итераций из 10000.\n",
      "Завершено 1000 итераций из 10000.\n",
      "Завершено 2000 итераций из 10000.\n",
      "Завершено 3000 итераций из 10000.\n",
      "Завершено 4000 итераций из 10000.\n",
      "Завершено 5000 итераций из 10000.\n",
      "Завершено 6000 итераций из 10000.\n",
      "Завершено 7000 итераций из 10000.\n",
      "Завершено 8000 итераций из 10000.\n",
      "Завершено 9000 итераций из 10000.\n",
      "Количество игр, данные по которым будут использованы для обучения:  577\n"
     ]
    }
   ],
   "source": [
    "x, y = nn_agent.data_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "114823/114823 [==============================] - 17s 146us/sample - loss: 0.2222 - acc: 0.3550\n",
      "Epoch 2/5\n",
      "114823/114823 [==============================] - 16s 143us/sample - loss: 0.2211 - acc: 0.3754\n",
      "Epoch 3/5\n",
      "114823/114823 [==============================] - 16s 143us/sample - loss: 0.2209 - acc: 0.3771\n",
      "Epoch 4/5\n",
      "114823/114823 [==============================] - 16s 144us/sample - loss: 0.2207 - acc: 0.3763\n",
      "Epoch 5/5\n",
      "114823/114823 [==============================] - 17s 147us/sample - loss: 0.2206 - acc: 0.3766\n"
     ]
    }
   ],
   "source": [
    "nn_agent.train_model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя награда за 100 игр: -146.71\n"
     ]
    }
   ],
   "source": [
    "model_test(nn_agent.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение №2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.env     = env\n",
    "        self.memory  = deque(maxlen=2000)\n",
    "        \n",
    "        self.gamma = 0.85\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.005\n",
    "        self.tau = .125\n",
    "\n",
    "        self.model        = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        model   = Sequential()\n",
    "        state_shape  = self.env.observation_space.shape\n",
    "        model.add(Dense(24, input_dim=state_shape[0], activation=\"relu\"))\n",
    "        model.add(Dense(48, activation=\"relu\"))\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(self.env.action_space.n))\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size: \n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = self.target_model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Failed to complete in trial 0\n",
      "Failed to complete in trial 1\n",
      "Failed to complete in trial 2\n",
      "Failed to complete in trial 3\n",
      "Failed to complete in trial 4\n",
      "Failed to complete in trial 5\n",
      "Failed to complete in trial 6\n",
      "Failed to complete in trial 7\n",
      "Failed to complete in trial 8\n",
      "Failed to complete in trial 9\n",
      "Failed to complete in trial 10\n",
      "Failed to complete in trial 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7ce23cc75575>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mdqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mdqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m# internally iterates default (prediction) model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mdqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# iterates target model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-30a38d32a50c>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mQ_future\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mQ_future\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32mD:\\Programms\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env     = gym.make(\"MountainCar-v0\")\n",
    "gamma   = 0.9\n",
    "epsilon = .95\n",
    "\n",
    "trials  = 1000\n",
    "trial_len = 500\n",
    "\n",
    "# updateTargetNetwork = 1000\n",
    "dqn_agent = DQN(env=env)\n",
    "steps = []\n",
    "for trial in range(trials):\n",
    "    cur_state = env.reset().reshape(1,2)\n",
    "    for step in range(trial_len):\n",
    "        action = dqn_agent.act(cur_state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # reward = reward if not done else -20\n",
    "        new_state = new_state.reshape(1,2)\n",
    "        dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "\n",
    "        dqn_agent.replay()       # internally iterates default (prediction) model\n",
    "        dqn_agent.target_train() # iterates target model\n",
    "\n",
    "        cur_state = new_state\n",
    "        if done:\n",
    "            break\n",
    "    if step >= 199:\n",
    "        print(\"Failed to complete in trial {}\".format(trial))\n",
    "        if step % 10 == 0:\n",
    "            dqn_agent.save_model(\"trial-{}.model\".format(trial))\n",
    "    else:\n",
    "        print(\"Completed in {} trials\".format(trial))\n",
    "        dqn_agent.save_model(\"success.model\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
